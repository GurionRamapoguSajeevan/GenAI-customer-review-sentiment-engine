{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GurionRamapoguSajeevan/GenAI-customer-review-sentiment-engine/blob/main/GenAI_sentiment_review_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: LIBRARIES AND INITIAL SET UP"
      ],
      "metadata": {
        "id": "hXqV5uI4zFuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required Libraries"
      ],
      "metadata": {
        "id": "O5lZrLxXsvfe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUxbupnrq4O_"
      },
      "outputs": [],
      "source": [
        "!pip install pandas nltk spacy scikit-learn transformers matplotlib seaborn streamlit\n",
        "!python -m spacy download en_core_web_sm  # For English NLP preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "B73leyOCs0QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from transformers import pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import streamlit as st\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "_PMDRYmAr7Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: Loading and Understanding the Data"
      ],
      "metadata": {
        "id": "GVSMEeZtzXF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/amazon_review.csv')"
      ],
      "metadata": {
        "id": "Uzd4-I6Ds3Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "YFzAGIwHvbhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "lZmc-G_XzkPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relevant columns: We'll focus on reviewText (for text analysis), overall (rating, as a proxy for sentiment validation), and asin (product ID, for filtering in the dashboard)."
      ],
      "metadata": {
        "id": "aoQaLtkRz6cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['reviewText', 'overall', 'asin']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "I6F73X0WzrJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Cleaning and Preprocessing the Text"
      ],
      "metadata": {
        "id": "XmjPmRxa1ICT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I am defining a preprocessing function (this handles lowercase, tokenization, stopword removal, and lemmatization—standard NLP steps to make text ready for models):"
      ],
      "metadata": {
        "id": "9yhh0gMu1w8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### this extra one is needed for the tabular data in the tokenizer. 'punkt_tab' tokenizer model required for word_tokenize in recent NLTK versions"
      ],
      "metadata": {
        "id": "5QAKyg-iKgjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "Vv_KvfKKt-nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. First, drop any rows where the actual reviewText is missing\n"
      ],
      "metadata": {
        "id": "n5O0ysjkKIpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['reviewText'])"
      ],
      "metadata": {
        "id": "99UJj-TjKRUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Define Preprocessing for LDA (Topic Modeling) ONLY"
      ],
      "metadata": {
        "id": "fOkmZSujKZ4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text_for_topics(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and non-alphanumeric (CRITICAL: Only do this for Topic Modeling, not Sentiment!)\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    lemmatized = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmatized)\n"
      ],
      "metadata": {
        "id": "EbuVkmPc0GTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Applying the Preprocessing Function to the reviewText column:"
      ],
      "metadata": {
        "id": "Zh68sDioKuZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_review'] = df['reviewText'].apply(preprocess_text_for_topics)"
      ],
      "metadata": {
        "id": "V6hotgtBJ6pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Filtering out rows that became empty after cleaning (so LDA doesn't crash), but keeping the index aligned for the raw text.\n"
      ],
      "metadata": {
        "id": "Ok74c8GKK0FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['cleaned_review'].str.len() > 0].copy()\n",
        "\n",
        "print(\"Data ready. 'reviewText' will be used for AI, 'cleaned_review' for LDA.\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "juUJ7CM6J9oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Extract Insights Using NLP and AI Models"
      ],
      "metadata": {
        "id": "MbaMDEwV4Qjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  1. Sentiment Analysis (using a lightweight DistilBERT model)"
      ],
      "metadata": {
        "id": "2w8qiCoy7Non"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# NEW MODEL: Uses a 5-star rating system which we map to 3 classes (Neg/Neu/Pos).\n",
        "\n",
        "sentiment_pipeline = pipeline(\n",
        "    'sentiment-analysis',\n",
        "    model='nlptown/bert-base-multilingual-uncased-sentiment',\n",
        "    device=0\n",
        ")\n",
        "\n",
        "# a. Prepare raw text list\n",
        "raw_reviews = df['reviewText'].astype(str).tolist()\n",
        "\n",
        "# b. Run inference in batches\n",
        "sentiment_results = sentiment_pipeline(raw_reviews, truncation=True, batch_size=16)\n",
        "\n",
        "# c. Define the mapping function to convert 5-star to 3-class label\n",
        "def map_star_to_sentiment(label):\n",
        "    if label in ['1 star', '2 stars']:\n",
        "        return 'NEGATIVE'\n",
        "    elif label == '3 stars':\n",
        "        return 'NEUTRAL'\n",
        "    elif label in ['4 stars', '5 stars']:\n",
        "        return 'POSITIVE'\n",
        "    # Handle potential label formatting (e.g., 'X star' vs 'X stars')\n",
        "    elif 'star' in label:\n",
        "        star_num = int(label.split()[0])\n",
        "        if star_num <= 2:\n",
        "            return 'NEGATIVE'\n",
        "        elif star_num == 3:\n",
        "            return 'NEUTRAL'\n",
        "        else:\n",
        "            return 'POSITIVE'\n",
        "    return 'NEUTRAL' # Default fallback\n",
        "\n",
        "# d. Extract labels and apply mapping\n",
        "df['sentiment'] = [map_star_to_sentiment(result['label']) for result in sentiment_results]\n"
      ],
      "metadata": {
        "id": "-8CF8_s83vF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# e. Vizualization check\n",
        "sns.countplot(x='sentiment', data=df)\n",
        "plt.title(\"Sentiment Distribution (3-Class)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qpvu7bbkuxNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking a sample to verify \"Positives\" aren't hallucinating on negative words\n",
        "print(df[['reviewText', 'sentiment']].head(10))"
      ],
      "metadata": {
        "id": "Su4R26xRuy96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Themes/Topics (using traditional LDA for simplicity and to show hybrid skills—AI + classic ML):"
      ],
      "metadata": {
        "id": "HJ-3Wbsp6ytA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "dtm = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # Extract 5 main themes\n",
        "lda.fit(dtm)\n",
        "\n",
        "# Display top words per theme (print in notebook)\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    print(f\"Theme {i}: {' '.join([vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-10:]])}\")\n",
        "\n",
        "# Assign dominant theme to each review\n",
        "df['theme'] = lda.transform(dtm).argmax(axis=1)"
      ],
      "metadata": {
        "id": "AD1TuhKY6sTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Pain Points and Suggestions (using zero-shot classification with BART: another GPU-accelerated HuggingFace model, ):"
      ],
      "metadata": {
        "id": "OUnKYc257ZlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "zero_shot_pipeline = pipeline(\n",
        "    'zero-shot-classification',\n",
        "    model='facebook/bart-large-mnli',\n",
        "    device=0\n",
        ")\n",
        "\n",
        "pain_labels = ['quality issue', 'delivery problem', 'price complaint', 'usability issue', 'no pain']\n",
        "suggestion_labels = ['improve durability', 'better packaging', 'add features', 'lower price', 'no suggestion']\n",
        "\n",
        "# Prepare raw text list\n",
        "raw_reviews = df['reviewText'].astype(str).tolist()\n",
        "\n",
        "print(\"Processing Pain Points... (This may take a moment)\")\n",
        "\n",
        "# Batch process Pain Points\n",
        "pain_results = zero_shot_pipeline(\n",
        "    raw_reviews,\n",
        "    candidate_labels=pain_labels,\n",
        "    batch_size=8, # BART is larger, so we use a smaller batch size to avoid Out of Memory\n",
        "    truncation=True\n",
        ")\n",
        "df['pain_point'] = [result['labels'][0] for result in pain_results]\n",
        "\n",
        "print(\"Processing Suggestions...\")\n",
        "# Batch process Suggestions\n",
        "suggestion_results = zero_shot_pipeline(\n",
        "    raw_reviews,\n",
        "    candidate_labels=suggestion_labels,\n",
        "    batch_size=8,\n",
        "    truncation=True\n",
        ")\n",
        "df['suggestion'] = [result['labels'][0] for result in suggestion_results]"
      ],
      "metadata": {
        "id": "uiG3Bvm-601H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Saving the Final Processed-reviewes Dataset"
      ],
      "metadata": {
        "id": "uFJEW65NLYZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final\n",
        "df.to_csv('/content/processed_reviews.csv', index=False)\n",
        "print(\"Processing complete. File saved.\")"
      ],
      "metadata": {
        "id": "UlMwa5ys7dDY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}