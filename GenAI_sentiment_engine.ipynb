{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFwnriHuns533LZN1LV9yd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GurionRamapoguSajeevan/GenAI-customer-review-sentiment-engine/blob/main/GenAI_sentiment_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: LIBRARIES AND INITIAL SET UP"
      ],
      "metadata": {
        "id": "hXqV5uI4zFuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required Libraries"
      ],
      "metadata": {
        "id": "O5lZrLxXsvfe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUxbupnrq4O_"
      },
      "outputs": [],
      "source": [
        "!pip install pandas nltk spacy scikit-learn transformers matplotlib seaborn streamlit\n",
        "!python -m spacy download en_core_web_sm  # For English NLP preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "B73leyOCs0QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from transformers import pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import streamlit as st\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "_PMDRYmAr7Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: Loading Data set"
      ],
      "metadata": {
        "id": "GVSMEeZtzXF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/amazon_review.csv')"
      ],
      "metadata": {
        "id": "Uzd4-I6Ds3Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "YFzAGIwHvbhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "lZmc-G_XzkPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relevant columns: We'll focus on reviewText (for text analysis), overall (rating, as a proxy for sentiment validation), and asin (product ID, for filtering in the dashboard)."
      ],
      "metadata": {
        "id": "aoQaLtkRz6cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['reviewText', 'overall', 'asin']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "I6F73X0WzrJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Cleaning and Preprocessing the Text"
      ],
      "metadata": {
        "id": "XmjPmRxa1ICT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I am defining a preprocessing function (this handles lowercase, tokenization, stopword removal, and lemmatization—standard NLP steps to make text ready for models):"
      ],
      "metadata": {
        "id": "9yhh0gMu1w8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = text.lower()  # Lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove stopwords and non-alphanumeric\n",
        "    doc = nlp(' '.join(tokens))  # Lemmatize with spaCy\n",
        "    lemmatized = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmatized)"
      ],
      "metadata": {
        "id": "EbuVkmPc0GTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### this extra one is needed for the tabular data in the tokenizer. 'punkt_tab' tokenizer model required for word_tokenize in recent NLTK versions"
      ],
      "metadata": {
        "id": "y__-SYto250n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "9RZt51xq2yHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the Preprocessing Function to the reviewText column:"
      ],
      "metadata": {
        "id": "ZEmJ73ij1Sul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_review'] = df['reviewText'].apply(preprocess_text)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ZfFloZx11-eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping empty reviews if any"
      ],
      "metadata": {
        "id": "x7ZCOU4u31EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['cleaned_review'])"
      ],
      "metadata": {
        "id": "-kBaytQO2Bmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Extract Insights Using NLP and AI Models"
      ],
      "metadata": {
        "id": "MbaMDEwV4Qjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1. Sentiment Analysis (using a lightweight DistilBERT model)"
      ],
      "metadata": {
        "id": "2w8qiCoy7Non"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8CF8_s83vF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', device=0)  # device=0 uses GPU\n",
        "\n",
        "# df['sentiment'] = df['cleaned_review'].apply(lambda x: sentiment_pipeline(x)[0]['label'] if x else 'NEUTRAL')"
      ],
      "metadata": {
        "id": "yHjb86zc4acz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'] = df['cleaned_review'].apply(lambda x: sentiment_pipeline(x, truncation=True, max_length=512)[0]['label'] if x else 'NEUTRAL')"
      ],
      "metadata": {
        "id": "Tcd8j8c-4fTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "id": "keypzYYu5YCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='sentiment', data=df)\n",
        "plt.show()  # Quick viz in notebook"
      ],
      "metadata": {
        "id": "f4ZCaJoV55qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Themes/Topics (using traditional LDA for simplicity and to show hybrid skills—AI + classic ML):"
      ],
      "metadata": {
        "id": "HJ-3Wbsp6ytA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "dtm = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # Extract 5 main themes\n",
        "lda.fit(dtm)\n",
        "\n",
        "# Display top words per theme (print in notebook)\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    print(f\"Theme {i}: {' '.join([vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-10:]])}\")\n",
        "\n",
        "# Assign dominant theme to each review\n",
        "df['theme'] = lda.transform(dtm).argmax(axis=1)"
      ],
      "metadata": {
        "id": "AD1TuhKY6sTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Pain Points and Suggestions (using zero-shot classification with BART—another HuggingFace model, GPU-accelerated):"
      ],
      "metadata": {
        "id": "OUnKYc257ZlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_pipeline = pipeline('zero-shot-classification', model='facebook/bart-large-mnli', device=0)  # GPU\n",
        "\n",
        "pain_labels = ['quality issue', 'delivery problem', 'price complaint', 'usability issue', 'no pain']\n",
        "suggestion_labels = ['improve durability', 'better packaging', 'add features', 'lower price', 'no suggestion']\n",
        "\n",
        "def extract_pain_point(text):\n",
        "    if not text:\n",
        "        return 'none'\n",
        "    result = zero_shot_pipeline(text, candidate_labels=pain_labels)\n",
        "    return result['labels'][0]\n",
        "\n",
        "def extract_suggestion(text):\n",
        "    if not text:\n",
        "        return 'none'\n",
        "    result = zero_shot_pipeline(text, candidate_labels=suggestion_labels)\n",
        "    return result['labels'][0]\n",
        "\n",
        "df['pain_point'] = df['cleaned_review'].apply(extract_pain_point)\n",
        "df['suggestion'] = df['cleaned_review'].apply(extract_suggestion)"
      ],
      "metadata": {
        "id": "uiG3Bvm-601H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/processed_reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "UlMwa5ys7dDY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}